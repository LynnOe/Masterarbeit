\subsection{Probleme mit Transparenz}
Bisher wurden in der Arbeit neben transparenzschaffenden Maßnahmen vor allem die positiven Aspekte von Transparenz im ML herausgestellt. Jedoch ergab die Literaturrecherche auch einige negative Aspekte von Transparenz oder deckte Bereiche auf, in denen noch Forschungs- oder Klärungsbedarf besteht.

\subsubsection{Offene Fragen bei Transparenz} 
\cite{zhou20182d} beschreiben das Konzept des transparenten ML (vgl. Kapitel \ref{subsec_Konzepte}) und führen hierbei zusätzlich offene Fragen an, die von ihrem Konzept noch nicht abgedeckt werden. So beschreiben sie, dass grundsätzlich Vertrauen bei transparenten ML zwar wichtig ist, jedoch wenig darüber bekannt ist, wie Vertrauen in Menschen entsteht. \cite{vaughan2020human} führen hierzu an, dass es vorkomme, dass Menschen einem ML-System mehr vertrauen, welches viele Features benutzt, obwohl sie es nicht verstehen. Ein Modell, welches sie verstehen, aber beispielsweise nur wenige Features nutze, führt zu weniger Vertrauen in das System.
Weiter bewege nach \cite{zhou20182d} Nutzer von ML-Systemen das Risiko bei der Nutzung eines solchen Systems. So seien diese Systeme nicht perfekt und ideal wäre eine Abwägung der Kosten, wenn auf Grundlage eines Systems in der realen Welt eine falsche Entscheidung getroffen wird. Weiter leiden bisherige XAI-Methoden vor allem unter einer Verallgemeinerung der Erklärungsempfänger. So bringen alle menschlichen Betrachter unterschiedliche Bildung, Kultur oder Geschlecht mit, was die Anforderungen an die Art und Weise der geforderten Kommunikation bedingt.
Besonders zu viel Transparenz und die Kommunikation besonders komplizierter Modelle sei nicht geeignet für Fachfremde, da hier das technische Hintergrundwissen nicht vorhanden ist \cite{de2018algorithmic}. Des Weiteren ist schwer zu entscheiden, welches Maß an Transparenz gut ist. Zu wenig Transparenz kann problematisch werden \cite{kulesza2013too} und wichtige Aspekte werden einfach ausgelassen \cite{blacklaws2018algorithms}. Zu viel Transparenz und zu viele Erklärungen führen jedoch dazu, dass Nutzer sehr viel Zeit und unter Umständen Wissensakkumulation dafür aufbringen müssen, die Erklärungen zu verstehen \cite{blacklaws2018algorithms}. \cite{blacklaws2018algorithms} sprechen hier von dem Transparenzparadoxon.
Auch \cite{biessmann2021turing} fanden heraus, dass das Erstellen von \enquote{guten und nutzerfreundlichen} Erklärungen nicht trivial ist. Sie führten einen Touringtest durch, in denen die Studienteilnehmer bestimmen sollten, ob eine vorliegende Erklärung (Markierung von 3 Buzz-Words bei einer Textklassifikation) von einem Menschen oder einer angewendeten XAI-Methode getroffen wurde. Ergebnis der Studie war, dass Menschen später nicht in der Lage waren, zu sagen, ob eine Klassifikationsentscheidung automatisiert oder von Hand getroffen wurde.

\subsubsection{Zeitaufwand}
Des Weiteren ist es laut \cite{vaughan2020human} vorgekommen, dass Nutzer das Erstellen eines mentalen Modells mithilfe von Erklärungen als Zeitverschwendung betrachteten. 

\subsubsection{Gefahren durch Transparenz}
Neben den bereits angeführten Problemen können einige Aspekte von Transparenz jedoch auch schwerwiegende Folgen in der Praxis haben, wenn Systeme z.B. in der Medizin oder im Verkehr eingesetzt werden. 
Erklärungen lassen sich z.B. manipulieren, sodass Nutzer den Erklärungen nicht mehr trauen können \cite{tjoa2020survey}. Ein Beispiel hierzu wird von \cite{ghorbani2019interpretation} beschrieben, das zeigt, wie aus einem originalen Bild ein für den Nutzer kaum zu unterscheidbares zweites Bild erstellt werden kann, dass jedoch eine gänzlich andere Erklärung erhält.
Neben funktionalen Problemen, welche kritisch für das Vertrauen und die korrekte Anwendung sind, bestehen noch weitere Probleme in Bezug auf Datenschutz \cite{de2018algorithmic}. Dies liegt auf der Hand, wenn wie in \todo[inline]{Hier verweise ich noch auf die Stelle (die habe ich nur noch nicht geschrieben :D)} beschrieben konkrete Beispiele oder Auszüge aus den Trainingsdaten offen gelegt werden, jedoch existieren auch subtilere Angriffe auf die Privatsphäre bei ML-Systemen. \cite{shokri2017membership} beschreiben die \enquote{Membership Inference Attack}, welche sich bei ML-Algorithmen anwenden lässt und es einem Angreifer ermöglicht herauszufinden, ob ein bestimmter Datenpunkt in dem genutzten Trainingsdatensatz vorhanden war.